{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "QOZS8mTKJfN8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing pybiblionet\n",
    "\n",
    "This notebook demonstrates the installation and basic usage of the pybiblionet package.  \n",
    "\n",
    "## Steps\n",
    "1. Install the library from PyPI.\n",
    "2. Import the library and gather articles.\n",
    "3. Show graphs generated by a query.\n",
    "4. Perform network analysis.\n",
    "\n"
   ],
   "metadata": {
    "id": "RjybCvKTKMG0"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#1. Install the library from PyPI.\n"
   ],
   "metadata": {
    "id": "YT06tsbrKkg3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install pybiblionet"
   ],
   "metadata": {
    "id": "YSFCfkhXKCbV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Import the library and gather articles.\n",
    "\n",
    "\n",
    "In this section, we need to specify the query parameters for retrieving data.  \n",
    "The process involves three main steps:\n",
    "\n",
    "1. **Query generation**  \n",
    "   A query string is created using a generator that follows a *lite regex–like approach*.\n",
    "\n",
    "2. **User identification (email)**  \n",
    "   You should provide your email address.  \n",
    "   This is required by OpenAlex to identify who is making the request and to help monitor rate limits.  \n",
    "   ⚠️ *No registration is required — just an email string.*\n",
    "\n",
    "3. **Time interval**  \n",
    "   Define the publication date range of interest by setting the start and (optionally) the end date.\n"
   ],
   "metadata": {
    "id": "EkHwJ7sXKvOk"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5EjjWNGzJQd9"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from pybiblionet.openalex.core import string_generator_from_lite_regex, retrieve_articles, create_citation_graph, \\\n",
    "    create_coauthorship_graph, export_articles_to_csv, export_authors_to_csv, export_venues_to_csv, \\\n",
    "    export_institutions_to_csv, export_articles_to_scopus\n",
    "\n",
    "# This script retrieves academic articles from OpenAlex related to the concept of \"15-minute city\"\n",
    "# using a regex-based query. It exports the retrieved articles and their authors to CSV files,\n",
    "# then generates and saves two graph representations: a citation graph and a co-authorship graph.\n",
    "# Finally, it prints the number of nodes and edges in each graph to give an overview of their structure.\n",
    "if __name__ == \"__main__\" :\n",
    "\n",
    "\n",
    "    queries = string_generator_from_lite_regex(\"(15)( )(minute|min)( )(city)\")\n",
    "\n",
    "    mail = \"youremail@domain.com\"\n",
    "    from_publication_date = \"2024-01-01\"\n",
    "    to_publication_date = None\n",
    "    json_file_path = retrieve_articles(queries, mail, from_publication_date, to_publication_date)\n",
    "\n",
    "    export_institutions_to_csv(json_file_path,\n",
    "                           fields_to_export=None,\n",
    "                           export_path=\"15minute_institutions.csv\"\n",
    "                           )\n",
    "\n",
    "    export_venues_to_csv(json_file_path,\n",
    "                           fields_to_export=None,\n",
    "                           export_path=\"15minute_venues.csv\"\n",
    "                           )\n",
    "\n",
    "    export_articles_to_csv(json_file_path,\n",
    "                           fields_to_export=None,\n",
    "                           export_path=\"15minute_articles.csv\"\n",
    "                           )\n",
    "    export_authors_to_csv(json_file_path,\n",
    "                           fields_to_export=None,#[\"author id\", \"author orcid\", \"author display_name\"],\n",
    "                           export_path=\"15minute_authors.csv\"\n",
    "                           )\n",
    "\n",
    "    articles = json.load(open(json_file_path))\n",
    "    G_citation=create_citation_graph(articles,\"15minute_citation_graph.GML\", base_set=True)\n",
    "\n",
    "    print(G_citation.number_of_nodes(),G_citation.number_of_edges())\n",
    "\n",
    "    G_coauthorship=create_coauthorship_graph(articles,\"15minute_coauthorship_graph.GML\", base_set=True)\n",
    "\n",
    "    print(G_coauthorship.number_of_nodes(),G_coauthorship.number_of_edges())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Show graphs generated by a query.\n",
    "\n",
    "In this section, based on the data previously collected, we demonstrate the method calls that enable the visualization of the available graphs.  \n",
    "\n",
    "It is important to specify the correct path for the output of the query results.  \n",
    "Note that the file name is automatically generated as a **hash of the search query**.  \n",
    "Therefore, for each different query, a new output file with a different name will be created.\n",
    "\n"
   ],
   "metadata": {
    "id": "SY5Y6_LgM7pI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import platform\n",
    "platform.system = lambda: \"Colab\"  # The plots are displayed using TkAgg on Linux (so on Linux, run sudo apt install python3-tk). For example, on Colab TkAgg cannot be used. Here in the notebook, I force the platform to appear as non-Linux.\n",
    "import matplotlib\n",
    "from IPython.display import Image, display\n"
   ],
   "metadata": {
    "id": "53RLy6QrXS-d"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from pybiblionet.bibliometric_analysis.charts import plot_topic_trends, plot_article_trends, \\\n",
    "    plot_top_authors, plot_top_keywords_from_abstracts, plot_keyword_trends\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import numpy as np\n",
    "import json\n",
    "# This script performs various bibliometric analyses on a set of academic articles stored in a JSON file (recovered using test_openalex.py).\n",
    "# It generates several visualizations:\n",
    "# 1. Article publication trends over time (by month, quarter, or year).\n",
    "# 2. Topic trends over time, highlighting root set and base set topics.\n",
    "# 3. Top authors based on citation count, focusing on the most cited authors.\n",
    "# 4. Top keywords extracted from article abstracts.\n",
    "# 5. Keyword trends over time, showing the frequency of keywords and their evolution.\n",
    "# The script uses `matplotlib` for plotting and supports customization for the time intervals, number of authors,\n",
    "# and other aspects of the analysis.\n",
    "if __name__ == \"__main__\" :\n",
    "\n",
    "\n",
    "    json_file_path= \"query_results/query_result_1ea020320b230de5a973a39682eaa53dce89a9bb026b441a5f825232.json\"\n",
    "    articles = json.load(open(json_file_path))\n",
    "\n",
    "\n",
    "\n",
    "    plot_article_trends(articles,\n",
    "                        date_from=datetime(2019, 1, 1),\n",
    "                        interval=\"month\",  # Change to \"month\" or \"quarter\" or \"year\" as needed\n",
    "                        date_to=datetime(2023, 12, 31),\n",
    "                        num_ticks=20)\n",
    "    plt.savefig(\"grafico.png\")\n",
    "    display(Image(\"grafico.png\"))\n",
    "\n",
    "    plot_topic_trends(\n",
    "        articles,\n",
    "        top_n_colors= [\"#a6cee3\",\"#1f78b4\",\"#b2df8a\",\"#33a02c\",\"#d7191c\"],\n",
    "        show_root_set=True,\n",
    "        show_base_set=True,\n",
    "        interval=\"quarter\", # Change to \"quarter\" or \"year\"\n",
    "        date_from=datetime(2019, 1, 1),\n",
    "        date_to=datetime(2022, 12, 31),\n",
    "        top_n=5,\n",
    "        )\n",
    "    plt.savefig(\"grafico.png\")\n",
    "    display(Image(\"grafico.png\"))\n",
    "    plot_top_authors(\n",
    "        articles,\n",
    "        date_from=datetime(2019, 1, 1),\n",
    "        date_to=datetime(2025, 12, 31),\n",
    "        num_authors=10,\n",
    "        by_citations=True,\n",
    "        show_base_set=True,\n",
    "        n_colors=[\"#a6cee3\",\"#1f78b4\",\"#b2df8a\",\"#33a02c\",\"#d7191c\",\"#fdae61\"]\n",
    "    )\n",
    "    plt.savefig(\"grafico.png\")\n",
    "    display(Image(\"grafico.png\"))\n",
    "\n",
    "    plot_top_keywords_from_abstracts(articles)\n",
    "    plt.savefig(\"grafico.png\")\n",
    "    display(Image(\"grafico.png\"))\n",
    "\n",
    "    plot_keyword_trends(\n",
    "        articles=articles,\n",
    "        date_from=datetime(2020, 1, 1),\n",
    "        date_to=datetime(2025, 1, 1),\n",
    "        show_root_set=True,\n",
    "        show_base_set=True,\n",
    "        top_n=10,\n",
    "        ngram_range=(1, 2),\n",
    "        interval=\"quarter\",\n",
    "        n_colors=[\"#a6cee3\", \"#1f78b4\", \"#b2df8a\", \"#33a02c\", \"#d7191c\", \"#fdae61\"]\n",
    "    )\n",
    "    plt.savefig(\"grafico.png\")\n",
    "    display(Image(\"grafico.png\"))\n"
   ],
   "metadata": {
    "id": "VoZYWgVYM67d"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Perform network analysis.\n",
    "\n",
    "\n",
    "In this final code section, we demonstrate how to invoke the methods that analyze the network using **NetworkX** and display the available visualizations.  \n",
    "As in the previous step, it is necessary to specify:  \n",
    "\n",
    "- the **output file** of the query, and  \n",
    "- the **path to the network in GML format**, which was generated in Step 1."
   ],
   "metadata": {
    "id": "KAStM5l4OGhc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pybiblionet.bibliometric_analysis.core import extract_metrics_to_csv, cluster_graph, extract_clusters_to_csv\n",
    "from pybiblionet.bibliometric_analysis.charts_network import show_clustered_graph, show_cluster_statistics,show_graph_statistics\n",
    "from matplotlib import colors\n",
    "\n",
    "# This script performs several operations on a citation graph of academic articles (recovered using test_openalex.py):\n",
    "# 1. Loads a graph from a GML file using `networkx`.\n",
    "# 2. Extracts a set of centrality and degree metrics from the graph (betweenness centrality, closeness centrality,\n",
    "#    eigenvector centrality, page rank, in-degree, and out-degree), and saves them along with additional article fields\n",
    "#    into a CSV file.\n",
    "# 3. Applies the Louvain clustering algorithm to the graph to detect communities (clusters) of articles.\n",
    "# 4. Saves the extracted cluster information (e.g., node IDs) to another CSV file.\n",
    "# 5. Visualizes the clustered graph using `show_clustered_graph`, showing different clusters of articles.\n",
    "# 6. Generates and displays a bar chart with statistics about the clusters, such as the number of nodes per cluster.\n",
    "# This analysis allows for exploring the structure of the network of academic citations and understanding the distribution\n",
    "# of various metrics and clusters within the data.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Read the graph\n",
    "\n",
    "    json_file_path=\"query_results/query_result_1ea020320b230de5a973a39682eaa53dce89a9bb026b441a5f825232.json\"\n",
    "    network_file_name= \"15minute_citation_graph.GML\"\n",
    "\n",
    "    G = nx.read_gml(network_file_name)\n",
    "    print(\"Graph loaded.\")\n",
    "    print(G.number_of_nodes(), G.number_of_edges())\n",
    "\n",
    "    # Extract and save metrics to CSV\n",
    "    metrics = [\"betweenness_centrality\",\n",
    "            \"closeness_centrality\",\n",
    "            #\"eigenvector_centrality\",\n",
    "            \"page_rank\",\n",
    "            \"in_degree\",\n",
    "            \"out_degree\"]\n",
    "    fields = [\"id\", \"title\",\"cited_by_count\",\"root_set\",\"primary_topic display_name\",\n",
    "                            \"primary_topic subfield display_name\",\n",
    "                            \"primary_topic field display_name\",\n",
    "                            \"primary_topic domain display_name\"]\n",
    "    csv_file_path = \"metrics_and_fields_citation.csv\"\n",
    "\n",
    "    print(\"Extracting metrics...\")\n",
    "    extract_metrics_to_csv(G, metrics, fields, csv_file_path)\n",
    "    #exit()\n",
    "    #show_graph_statistics(G,csv_file_path)\n",
    "\n",
    "    print(\"Metrics extracted and saved to CSV.\")\n",
    "\n",
    "    print(\"\\nClustering the graph...\")\n",
    "    fields = [\"id\"]\n",
    "    clustered_graph = cluster_graph(G, algorithm='louvain',)\n",
    "    csv_file_path = \"cluster_and_fields_citation.csv\"\n",
    "    extract_clusters_to_csv(clustered_graph, fields, csv_file_path)\n",
    "    #exit()\n",
    "    print(\"Clusters extracted and saved to CSV.\")\n",
    "    # Visualize clusters\n",
    "    print(\"Visualizing clusters...\")\n",
    "    show_clustered_graph(clustered_graph, image_size=(800, 800),\n",
    "                         n_clusters=5,\n",
    "                         topics_level=\"field\",\n",
    "                         #min_node_radius=3,\n",
    "                         #max_node_radius=5,\n",
    "                         #max_pie_radius=5,\n",
    "                         #min_pie_radius=3,\n",
    "                         #size_node_font=12,\n",
    "                         export_path_png=\"grafico.png\"\n",
    "                         )\n",
    "    display(Image(\"grafico.png\"))\n",
    "\n",
    "    show_cluster_statistics(csv_file_path, image_size=(800, 800),\n",
    "                 n_clusters=5)\n",
    "    plt.savefig(\"grafico.png\")\n",
    "    display(Image(\"grafico.png\"))"
   ],
   "metadata": {
    "id": "TFctrZUEOGwI"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
